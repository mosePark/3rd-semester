논문 제목 : "Improving Out-of Distribution Robustness via Selective Augmentation"

# 문제점

1. 분포 차이에 따른 성능 저하
- 기계학습 알고리즘은 일반적으로 훈련 데이터와 시험 데이터가 같은 분포에서 추출된다고 가정
- 실세계 응용에서는 훈련 데이터와 시험 데이터 간 분포가 달라지는 경우가 흔하며, 이는 모형의 성능 저하를 초래
- 이러한 분포 차이는 subpopulation shift와 domain shift와 같은 형태로 나타날 수 있음

2.기존 연구의 한계
- 기존 연구들은 주로 모델의 internel representation이나 예측기를 domain invariant하게 규제화하는 방법에 중점을 둚
- 이러한 규제화 방법은 복잡하며, 반드시 성능 향상을 보장 (X)

# 해결책
1. LISA: 선택적 증강 방법 제안
- 본 논문에서는 internel representation이나 예측기를 규제하지 않고도 invariant한 예측기를 학습하는 새로운 방법을 제안
- 이를 위해 mix-up을 기반으로 한 선택적 증강 기법인 LISA (Label Invariant Selective Augmentation)를 도입

2. LISA의 주요 개념
내삽 (Mix-up): 같은 라벨이지만 다른 도메인의 샘플, 혹은 같은 도메인에 있으나 다른 라벨인 샘플을 내삽
선택적 증강: 특정 조건을 만족하는 샘플들 간의 mix-up을 통해 데이터 증강을 수행
라벨 불변 (Label Invariance): 내삽된 샘플은 여전히 의미 있는 라벨을 유지하여 모델이 도메인 간의 차이를 극복하고 일반화된 예측을 할 수 있도록 함

3. LISA의 사용 방법
  3.1 데이터 준비: 다양한 도메인과 라벨을 가진 샘플들을 준비한다.
  3.2 샘플 선택: 같은 라벨이지만 다른 도메인에 속한 샘플들, 또는 같은 도메인에 있으나 다른 라벨을 가진 샘플들을 선택한다.
  3.3 Mix-up 적용: 선택된 샘플들을 내삽하여 새로운 증강 샘플을 생성한다. 이 과정에서 원본 샘플의 특징을 적절히 혼합하여 새로운 학습 데이터를 만든다.
  3.4 모델 학습: 생성된 증강 샘플들을 이용하여 모델을 학습시킨다. 이로 인해 모델은 도메인 간 차이를 극복하고 일반화된 성능을 보일 수 있다.
