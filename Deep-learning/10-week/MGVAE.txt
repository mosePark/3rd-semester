논문 제목 : "Generative Oversampling for Imbalanced Data via Majority-Guided VAE"

# 문제점
  - 클래스간 샘플의 불균형 → 특정 클래스에 편향이 생김
  - 기존 오버샘플링의 경우 소수 클래스 데이터만을 늘리는데 집중 → 실질적인 해결책이 아닐 수 있음

# 해결책
  - Majority-Guided VAE(MGVAE)
    - 다수 클래스 데이터를 기반으로 새로운 소수 클래스 샘플을 생성 → 다수 클래스의 사전 분포를 활용해 생성
  - pre trained, finetuning
    - MGVAE는 먼저 충분한 다수 클래스 샘플로 사전 학습을 수행하고, 이후 소수 클래스 샘플을 기반으로 미세 조정(fine-tuning)을 진행
  - Elastic Weight Consolidation(EWC) 정규화
    - EWC 정규화를 사용하여 파인 튜닝할때 모델의 붕괴를 방지
     → "파인 튜닝 과정에서 모델의 붕괴가 왜 일어나고 어떻게 일어나는지 이해를 못함."

# 느낀점 : 논문을 읽으면서 oversampling인 SMOTE 기법이 많이 생각났습니다. 그리고 대조학습, GAN, VAE의 학습방식의 기조가 뭔가 비슷하다는 느낌을 받았습니다.
확실히 오버샘플링 기법을 활용한다고 해서 꼭 좋은 결과가 나오진 않지만 이 논문에서는 그 부분을 꼬집어 해결했다는 점이 인상깊었습니다. 추가로 궁금한 부분은 prior에서
샘플링을 했다고 하는데 prior를 어떻게 찾았는지?
