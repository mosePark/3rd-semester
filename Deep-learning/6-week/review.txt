Attention and Transformer

% 강의자료와 위키독스의 딥러닝을 이용한 자연어 처리 입문 참고

# RNN 요약
  구조 : ℎ𝑡 =𝜎(𝑊*𝑥_𝑡 +𝑈*ℎ_{𝑡−1}+𝑏)
  이전 은닉층의 정보를 다음 은닉층에서 활용, U도 역전파를 거쳐 갱신해야함

# RNN의 유형
- One-to-Many
  하나의 입력을 받아 여러 개의 출력을 예측
  예를 들어, 이미지 캡셔닝에서는 하나의 이미지를 주고 그 이미지를 설명하는 여러 단어(즉, 문장)를 생성
  
- Many-to-One
  여러 입력을 받아 하나의 출력을 예측
  예를 들어, 감정 분석에서는 여러 단어(즉, 문장)를 입력으로 받아 하나의 감정을 예측
  
- Many-to-Many
  여러 입력을 받아 여러 출력을 예측
  예를 들어, 기계 번역에서는 한 언어의 여러 단어(즉, 문장)를 입력으로 받아 다른 언어의 여러 단어(즉, 문장)로 번역
  시퀀스-투-시퀀스(Seq2Seq) 모델은 전체 시퀀스 입력 후 출력을 생성하는 형태로, 예를 들어 문장 생성에서는 입력 단어를 주고 다음 단어를 예측

Seq2Seq

  Seq2Seq 모델은 인코더와 디코더로 구성됩니다.
  인코더: 입력 시퀀스를 고정 크기의 컨텍스트 벡터로 압축합니다.
  디코더: 이전 출력을 바탕으로 새로운 출력 시퀀스를 생성합니다.
  티처 포싱: 디코더에 모델 자체의 예측 출력 대신 실제 출력("je suis etudiant")이 제공됩니다.
  Seq2Seq 모델 (계속)
  
  인코더의 은닉 상태를 𝒉𝟏, 𝒉𝟐, ... 𝒉𝑵으로 표시합니다.
  인코더로부터의 컨텍스트 벡터를 𝒂로 표시합니다.
  따라서 𝒂=𝒉𝑵, 즉 아래 구조에서 𝑎=ℎ4와 같습니다.
  마지막 은닉 상태 𝒉𝑁은 입력 시퀀스 전체를 요약합니다.
  디코더의 은닉 상태를 𝒔𝟏, 𝒔𝟐, ... 𝒔𝑻로 표시합니다.

# Seq2Seq 모델의 한계점과 해결책
  - 컨텍스트 병목 현상(Context Bottleneck)
  - 정보 손실

  * 디코더의 각 시간 단계마다 컨텍스트 벡터를 생성
  * 인코더의 모든 은닉 상태를 포함하여 컨텍스트 벡터를 생성

# Attention 메커니즘
  - 위 두가지 문제점을 해결하고자 제안
  - 각 디코딩 단계에서 입력 시퀀스의 모든 부분에 대해 가중치를 조정함으로써 중요한 정보에 더 많은 주의를 기울이고, 덜 중요한 정보는 덜 고려하는 방식으로 작동

  * 가중치 a 결정 방법
   - α_i는 현재 디코딩 단계에서 은닉 상태 h_i의 중요도를 나타내야 함
   - h_i의 중요도는 현재 디코딩 단계의 입력 s_t와의 유사성으로부터 얻을 수 있음
   - ∑_i α_i는 1이 되어야 하며, 이는 소프트맥스(softmax) 함수를 사용함으로써 달성


  4가지 단계를 다음과 같이 진행
  1. 어텐션 스코어를 구한다.
  2. 소프트맥스(softmax) 함수를 통해 어텐션 분포(Attention Distribution)를 구한다.
  3. 각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값(Attention Value)을 구한다.
  4. 어텐션 값과 디코더의 t 시점의 은닉 상태를 연결한다. (concat)
  5. 출력층 연산의 입력이 되는 s_tilda를 계산합니다.

  ## 다양한 종류의 score
    - doc-prod
    - scaled dot-product
    - Bilinear
    - Multi-layer (General)

Transformer
  - attention 이 있는 seq2seq 모델의 한계점을 극복하고자 제안
    - Sequential processing : 각 은닉 상태는 이전 정보만을 포함
    - Long-term Dependency : 리 떨어진 과거의 입력 정보 대부분은 현재 은닉 상태에서 잊혀짐

# self-attention
  1. Score("I", "I") 계산하기
    1-1. 필요한 요소 준비
    1-2. attention score 계산
  2. attention score 계속 계산
    - 𝑠𝑐𝑜𝑟𝑒𝑄,𝐾 = 𝑄𝐾𝑇 / 𝑑𝑘**0.5
  3. softmax 적용
  4. attention 분포를 value vector에 적용

# transformer 아키텍처
  ## Encoders
    1. input embedding, positional encoding
      - 인풋 임베딩의 경우 512차원 (논문에서 제안)
      - 위치 임베딩은 단어의 위치에도 정보를 포함하고 있기 때문에 임베딩하는 것, sin() cos() 로 계산

    2. encoder self-attention
      - 각각의 단어 임베딩 벡터를 self-attention을 통해 갱신
      - multi-head attention을 통해 각각 나온 attention value들을 concat해줌
      - 업데이트된 값 벡터들은 ReLU 활성화 함수를 사용하는 2개의 MLP(다층 퍼셉트론) 레이어로 구성된 피드포워드 네트워크를 통과
      - 마지막으로, 전체 입력 시퀀스를 요약하는 컨텍스트 벡터를 얻음
      * 이 컨텍스트 벡터는 디코더로 전달되어 출력을 생성합니다.
      * 구체적으로, 최종 키(Key)와 값(Value)이 디코더로 전달됩니다.
      
  ## Decoders
    1. output embedding, positional encoding
    2. mask된 decoder self-attention
      "I am a student"를 "Je suis étudiant"로 번역하는 경우를 고려해 봅시다.
      예를 들어, "étudiant"를 예측한다고 가정해 봅시다.
      이는 "<SOS> Je suis"만 주어진 상태에서 "étudiant"를 예측해야 함을 의미합니다.
      디코더의 자기 주의를 계산할 때 미래의 단어를 고려하는 것은 종류의 부정 행위가 됩니다.
      따라서 자기 주의 계산 시 미래의 단어를 마스크 처리해야 합니다.
      미래 단어의 주의 점수에 큰 음수 값을 설정하면 소프트맥스 함수 적용 시 그 값이 0이 됩니다.
    3. Encoder-Decoder Attention
      이제 출력 시퀀스에 대해 업데이트된 값 벡터를 자기 주의를 사용하여 가지고 있습니다.
      입력 시퀀스에서 정보를 통합하기 위해, 인코더의 키와 값, 디코더의 쿼리를 사용합니다.
      이를 통해 디코더가 디코딩하는 동안 입력 시퀀스의 다양한 부분에 attention을 기울일 수 있습니다.


    4. Output Probabilities
      - 디코더의 출력은 선형 및 소프트맥스 함수를 통해 단어의 확률을 생성
      - 단어 확률이 가장 높은 값이 할당
