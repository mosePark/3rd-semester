\documentclass[10pt, aspectratio=169]{beamer}

\usetheme[progressbar=frametitle]{metropolis}
\usecolortheme{aggie}

\usepackage{appendixnumberbeamer}
\usepackage{tcolorbox} % 내 설정
\tcbuselibrary{breakable}
\usepackage{booktabs}
\usepackage[scale=2]{ccicons}


\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{ltablex}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{booktabs}
\usepackage{array}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usetikzlibrary{positioning}



\usepgfplotslibrary{dateplot}

\usepackage{xspace}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}
\newcommand{\printfnsymbol}[1]{%
  \textsuperscript{\@fnsymbol{#1}}%
}
\makeatother
\newenvironment{callig}{\fontfamily{qcr}\selectfont}{}
\newcommand{\textcallig}[1]{{\callig#1}}
\newcommand{\f}{v}
\newcommand{\g}{g}
\newcommand{\hY}{\hat{y}}
\newcommand{\x}{x}
\newcommand{\z}{z}
\newcommand{\I}{\delta}
%\newcommand{\R}{\mathbb{R}}
\newcommand{\expd}{\text{\textcallig{p}}}
\newcommand{\ex}{\Expl}
\newcommand{\prodd}{\Upsilon}
\def\Expl{\mathcal{E}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\m}{m}
\newcommand{\M}{M}
\newcommand{\bfalpha}{\mathbf{\alpha}}
\newcommand{\veck}{\mathbf{k}}
\newcommand*{\medcup}{\mathbin{\scalebox{1.5}{\ensuremath{\cup}}}}
\newcommand{\mubar}[1]{\bar{\mu}_{#1}}
\newcommand{\setlessell}{\mathcal{S}_\ell }
\newcommand{\bfb}{\mathbf{b} }
\DeclareMathOperator{\spann}{span}
\DeclareMathOperator{\calA}{\mathcal{A}}
\newcommand*{\defeq}{\stackrel{\text{def}}{=}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}



% start == Title Page %%%
\title{Multiple Testing}
\subtitle{In Nonparametric Hidden Markov Models:
 An Empirical Bayes Approach}
% \date{\today}
\date{June 4, 2024}
\author{mose Park}
\institute{Department of Statistical Data Science \\
    University of Seoul}
\vfuzz=20pt
\hfuzz=10pt
% end == Title Page %%%

%%% box
% 사용자 정의 tcolorbox 환경을 만듭니다.
\newtcolorbox{assumpbox}[2][]{%
    title=#2,
    colback=white,
    colframe=red!50!black,
    coltitle=white,
    fonttitle=\bfseries,
    rounded corners,
    boxsep=5pt,
    boxrule=1pt,
    #1 % 추가 옵션을 위한 공간
}

\newtcolorbox{mydefbox}[2][]{%
    title=#2,
    colback=white,
    colframe=red!50!gray, % 여기에 오타 수정
    coltitle=white,
    fonttitle=\bfseries,
    rounded corners,
    boxsep=5pt,
    boxrule=1pt,
    #1 % 추가 옵션을 위한 공간
}

\newtcolorbox{mytheorembox}[2][]{%
    title=#2,
    colback=white,
    colframe=red!50!gray, % 여기에 오타 수정
    coltitle=white,
    fonttitle=\bfseries,
    rounded corners,
    boxsep=5pt,
    boxrule=1pt,
    #1 % 추가 옵션을 위한 공간
}

%%%
\begin{document}

\maketitle

%===
\begin{frame}{Table of contents}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents%[hideallsubsections]
\end{frame}
%===
\section[Introduction]{Introduction}
%===
%===
\begin{frame}{Benchmark study}
    \begin{figure}[h]
        \centering
        \includegraphics[width=1.0\textwidth]{fig-2/relatework.png}
    \end{figure}


    \begin{itemize}[label=\scalebox{0.5}{$\blacksquare$}]
        \vspace{1.5em}
        \itemsep1.2em
        \item For example, There are a high correlation in a microarray data.
        \item This paper approaches the problem \textit{nonparametrically}. (empirical nayes method in HMM)
    \end{itemize}
    % 참고 자료 추가
    \begin{tikzpicture}[remember picture,overlay]
        \node[anchor=south west] at (current page.south west) {%
            \scalebox{0.5}{* Sun, W. \& Cai, T. T. (2009)}
        };
    \end{tikzpicture}
\end{frame}
%===
%===
\begin{frame}{Aim of the paper}
    \begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{fig-2/aim.png}
    \end{figure}

    \begin{itemize}[label=\scalebox{0.5}{$\blacksquare$}]
        \vspace{1.5em}
        \itemsep1.2em
        \item $ "\theta = 0" $ means \textbf{typical disease} levels. → ex) \textit{A common cold}
        \item $ "\theta = 1" $ means \textbf{atypical outbreak}. → ex) \textit{MERS, Covid-19}
    \end{itemize}
\end{frame}
% 데이터 X, 마코프 체인으로부터 추출된 관찰되지 않은 범주형 변수 theta (0,1)
% 독립성 기반 가정 고려 X
% hidden state theta로부터 생성된 obs data X 
%===
\begin{frame}
    \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
        \item \textbf{Thresholding procedure} maintain \textbf{optimality properties} in multiple testing.
        \vspace{0.6em}
        \begin{itemize}[label=\scalebox{0.5}{$\circ$}]
            \item \textbf{Empirical Bayesian procedure} in non-HMM achieves \textbf{target FDR} and \textbf{TDR}.
            \vspace{0.6em}
            \item \textbf{Controlling \(\ell\)-value} using \textbf{sup-norm estimators}.
            \vspace{0.6em}
            \item Achieving the target convergence rate.
        \end{itemize}
        \vspace{0.6em}
        \item Advantages of Nonparametric HMM Modeling:
        \vspace{0.6em}
        \begin{itemize}[label=\scalebox{0.5}{$\star$}]
            \item Arbitrary distributions under null hypothesis.
        \end{itemize}
    \end{itemize}
\end{frame}
% 수정할부분좀 수정해보기
% 


%===
%===
\begin{frame}{'HMM' setting}
    \begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{fig-2/HMM.png}
    \end{figure}
\end{frame}
% 추가 설명은 (X, theta)의 law를 라지 파이(joint), marginal law도 포함
%===
%===
\begin{frame}{'Multiple test' setting}
\begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
    \item The goal of multiple testing : to create a procedure \(\varphi = \varphi(X)\) that accurately identifies signals (\(\theta_i = 0\)).
    \item Performance is measured for all hypotheses \(i = 1, \ldots, N\) using FDR and TDR at \(\theta\) is defined as:
\end{itemize}

\begin{align}
    \text{FDP}_\theta(\varphi) := \frac{\sum_{i=1}^N \mathbf{1}\{\theta_i = 0, \varphi_i = 1\}}{1 \vee \left(\sum_{i=1}^N \varphi_i\right)} \tag{2}
\end{align}

with \(\text{FDR}_\theta(\varphi) := \mathbb{E}_H[\text{FDP}_\theta(\varphi(X)) | \theta]\).

\end{frame}
%===
%===
\begin{frame}

Consider the avg(FDR) for \(\theta\) generated by “prior” law \(\Pi_H\):

\[
\text{FDR}_H(\varphi) := \mathbb{E}_{\theta \sim \Pi_H} \left[ \text{FDR}_\theta(\varphi) \right] \equiv \mathbb{E}_H \left[ \text{FDP}_\theta(\varphi) \right] \tag{3}
\]

And define the ‘posterior FDR’ as the avg(FDP) obtained when \(\theta\) is drawn from posterior:

\[
\text{postFDR}_H(\varphi) = \text{postFDR}_H(\varphi;X) := \mathbb{E}_H\left[\text{FDP}_\theta(\varphi) \mid X\right] \tag{4}
\]

TDR is defined as $\mathbb{E}(\text{proportion of signals})$ detected by a $\varphi$:

\[
\text{TDR}_H(\varphi) = \mathbb{E}_H \left[ \frac{\sum_{i=1}^N \mathbf{1}\{\theta_i = 1, \varphi_i = 1\}}{1 \vee \left(\sum_{i=1}^N \theta_i\right)} \right] \tag{5}
\]

\vspace{2em}
\begin{itemize}[label=\scalebox{0.5}{$\blacksquare$}]
    \item Because we don't know parameter, we use empirical bayesian method.
\end{itemize}
\end{frame}
% 프로시저가 1이라는 것은 무슨 의미일까?
%===

\section[Empirical approach]{Empirical Bayesian Procedure}


%===
\begin{frame}{Thresholding procedure}
    
    \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
        \item Based on thresholding by posterior probabilities as ‘$\ell$-values’:
        \begin{equation}
            \ell_i(X) \equiv \ell_{i,H}(X) = \Pi_H(\theta_i = 0 | X). \tag{7}
        \end{equation}
        
        \item In the parameter \( H \) is known, an $\ell$-value thresholding procedure is the optimal procedure:
        \begin{equation}
            \varphi_{\lambda,H}(X) = \left(1\{\ell_{i,H}(X) < \lambda\}\right)_{i \leq N}. \tag{8}
        \end{equation}
        
        \item When the parameter \( H \) is unobserved, an estimator \( \hat{H} \) is used:
        \begin{equation}
             \hat{\lambda} = \hat{\lambda}(\hat{H},t) := \sup\{\lambda : \text{postFDR}_{\hat{H}}(\varphi_{\lambda,\hat{H}}) \leq t\}. \tag{9}
        \end{equation}
    \end{itemize}
    \vspace{2em}
    \begin{itemize}[label=\scalebox{0.5}{$\blacksquare$}]
        \item Although the procedure is similar to the BH procedure, the BH procedure utilizes p-values.
    \end{itemize}
\end{frame}
%===
%===
\begin{frame}
    \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
        \item Alternative characterisation of the threshold \( \hat{\lambda} \).
        \item By def (5) and (7), postFDR following: 
        \begin{equation}
            \text{postFDR}_H(\varphi) = \frac{\sum_{i=1}^{N} \ell_{i,H} \varphi_i}{1 \lor (\sum_{i=1}^{N} \varphi_i)}. \tag{10}
        \end{equation}
        \item The threshold \( \hat{\lambda} \) can be equivalently expressed as \( \hat{\lambda} = \hat{\ell}_{(\hat{K}+1)} \) and \( \hat{K} \) is defined by:
        \begin{equation}
            \frac{1}{\hat{K}} \sum_{i=1}^{\hat{K}} \hat{\ell}_{(i)} \leq t < \frac{1}{\hat{K}+1} \sum_{i=1}^{\hat{K}+1} \hat{\ell}_{(i)}. \tag{11}
        \end{equation}
        \item \( \hat{K} \) is well-defined and unique by monotonicity of the average of non-decreasing numbers.
        \item Therefore, following Dichotomy:
        \begin{equation}
            \text{postFDR}_{\hat{H}}(\varphi_{\lambda, \hat{H}}) \leq t \iff \lambda \leq \hat{\lambda}. \tag{12}
        \end{equation}
    \end{itemize}
\end{frame}
% 왜 람다 햇을 채택하는지? 왜 대체하는지?
% 여기 파트 디테일이 너무 어렵다.
%===
%===
\begin{frame}{Rejection set}
    \begin{mydefbox}{Definition 1.}
        Define \( \hat{\varphi} = \hat{\varphi}(t) \) to be a procedure rejecting exactly \( \hat{K} \) of the hypotheses with the smallest \( \hat{\ell}_i \) values, choosing arbitrarily in case of ties, where \( \hat{K} \) is defined by (11). We write \( \hat{S}_0 \) for the rejection set
        \[
        \hat{S}_0 = \{i \leq N : \hat{\varphi}_i = 1\},
        \]
        and we note that by construction we have \( |\hat{S}_0| = \hat{K} \) and
        \[
        \{i : \hat{\ell}_i(X) < \hat{\lambda} \} \subseteq \hat{S}_0 \subseteq \{i : \hat{\ell}_i(X) \leq \hat{\lambda} \}.
        \]
    \end{mydefbox}
    \begin{itemize}[label=\scalebox{0.5}{$\blacksquare$}]
    \item Last term means that the procedure \(\hat{\varphi}\) includes all hypotheses with \(\hat{\ell}_i\) values smaller than the threshold \(\hat{\lambda}\), and may include hypotheses with \(\hat{\ell}_i\) values equal to \(\hat{\lambda}\).
    \end{itemize}

\end{frame}
%===
%===
\begin{frame}{Assumption}
    \begin{assumpbox}{Assumption A}
        There exists \( x^* \in \mathbb{R} \cup \{\pm\infty\} \) such that either
        \[
        \frac{f_1(x)}{f_0(x)} \to \infty, \quad \text{as } x \uparrow x^*,
        \]
        or
        \[
        \frac{f_1(x)}{f_0(x)} \to \infty, \quad \text{as } x \downarrow x^*,
        \]
        where we take the conventions that \( \frac{1}{0} = \infty \), \( \frac{0}{0} = 0 \).
    \end{assumpbox}
    \begin{itemize}[label=\scalebox{0.5}{$\blacksquare$}]
        \item Signal Strength Assumption
        \item Weak signals → Hard FDR control, power ↓ Heller and Rosset (2021)
    \end{itemize}
\end{frame}
%===
%===
\begin{frame}
    \begin{assumpbox}{Assumption B}
        There exists a constant \( \nu > 0 \) such that
        \begin{equation*}
            \max_{j=0,1} \mathbb{E}_{X \sim f_j} (|X|^\nu) < \infty.
        \end{equation*}
    \end{assumpbox}
     \begin{assumpbox}{Assumption C}
        \begin{itemize}
            \item The matrix \( Q \) has full rank (i.e., its two rows are distinct), and 
            \[
            \delta := \min_{i,j} Q_{i,j} > 0.
            \]
            \item The Markov chain is stationary: the initial distribution \( \pi = (\pi_0, \pi_1) \) is the invariant distribution for \( Q \).
        \end{itemize}
    \end{assumpbox}
    \begin{itemize}[label=\scalebox{0.5}{$\blacksquare$}]
        \item B : moment condition (used in Lemma 12, 31)
        \item C : Required for identifiability for permutations
    \end{itemize}
\end{frame}
% full rank가 중요한 이유? : 정확한 추정을 하기 위해서, 선형독립이어야 해가 유일하게 존재하는 것을 보장할 수 있기 때문
%===
\begin{frame}{Asymptotic properties}
   \begin{mytheorembox}{Theorem 2.}
        Grant Assumptions A to C. Suppose that for some \( u > 1 + \nu^{-1} \) and some sequence
        \( \varepsilon_N \) such that \( \varepsilon_N (\log N)^u \to 0 \), the estimators \( \hat{Q}, \hat{\pi} \) and \( \hat{f}_j, j = 0,1 \) satisfy
        \[
        \Pi_H \left( \max \left\{ \|\hat{Q} - Q\|_F, \|\hat{\pi} - \pi\|, \|\hat{f}_0 - f_0\|_\infty, \|\hat{f}_1 - f_1\|_\infty \right\} > \varepsilon_N \right) \to 0, \quad \text{as } N \to \infty.
        \]
        Then for \( \hat{\varphi} \) the multiple testing procedure of Definition 1 we have
        \[
        \text{FDR}_H(\hat{\varphi}) \to \min(t, \pi_0).
        \]
   \end{mytheorembox}
   \vspace{2em}
    \begin{itemize}[label=\scalebox{0.5}{$\blacksquare$}]
        \item Ensuring \textbf{asymptotic properties} of the estimators.
    \end{itemize}
\end{frame}
%===
%===
\begin{frame}
   \begin{mytheorembox}{Theorem 3.}
        In the setting of Theorem 2, additionally grant that the distribution function of the
        random variable \( \left( \frac{f_1}{f_0} \right) (X_1) \) (i.e. the function \( t \mapsto \Pi_H \left( \left( \frac{f_1}{f_0} \right) (X_1) \leq t \right) \) is continuous and strictly
        increasing. Then the procedure \( \hat{\varphi} \) of Theorem 2 satisfies the following as \( N \to \infty \):
        \begin{equation*}
            \begin{split}
            \text{mTDR}_H(\hat{\varphi}) = &\sup \{ \text{mTDR}_H(\psi) : \text{mFDR}_H(\psi) \leq \text{mFDR}_H(\hat{\varphi}) \} + o(1) \\
            = &\sup \{ \text{mTDR}_H(\psi) : \text{mFDR}_H(\psi) \leq t \} + o(1).
            \end{split}
        \end{equation*}
        The suprema are over all multiple testing procedures \( \psi \) satisfying the bound on their mFDR, including
        oracle procedures allowed knowledge of the parameters \( H \).
    \end{mytheorembox}
    \begin{itemize}[label=\scalebox{0.5}{$\blacksquare$}]
    \item Maintains the mFDR below a given level while nearly maximizing the mTDR.
    \end{itemize}
\end{frame}
%===
\section[Estimation of Emission Densities]{Estimation of emission densities}
%===
\begin{frame}{Convergence rate}
   \begin{assumpbox}{Assumption C'}
        The matrix \( Q \) is full rank, the \( J \)-state Markov chain \( (\theta_n)_{n \in \mathbb{N}} \) is irreducible and aperiodic, and \( \theta_1 \) follows the invariant distribution. [This is weaker than Assumption C in general, but equivalent in the two-state setting.]
    \end{assumpbox}

    \begin{assumpbox}{Assumption D}
        The density functions \( f_1, \ldots, f_J \) are linearly independent. [In the two-state setting it suffices to assume \( f_0 \neq f_1 \), which is implied by Assumption A.]
    \end{assumpbox}
    \begin{itemize}[label=\scalebox{0.5}{$\blacksquare$}]
    \item C' : Ensures that all hidden states influence the data and applies general Markov chain convergence.
    \end{itemize}
\end{frame}
%===
%===
\begin{frame}
   \begin{mytheorembox}{Theorem 4}
        Assume that the dominating measure \( \mu \) is the counting measure on \( \mathbb{Z} \). Let \( M_N \) be a sequence tending to infinity, arbitrarily slowly. Under Assumptions C' and D, there exist estimators \( \hat{f}_1, \ldots, \hat{f}_J \) and a permutation \( \tau \) such that
        \[
        \Pi_H \left( \| \hat{f}_j - f_{\tau(j)} \|_\infty \geq M_N N^{-1/2} \right) \to 0.
        \]
    \end{mytheorembox}
    \begin{itemize}[label=\scalebox{0.5}{$\blacksquare$}]
    \item counting measure : discrete setting.
    \item $\tau$ : Since label switching issue.
    \end{itemize}
\end{frame}
% tau가 왜 필요한지? 설명이 좀 필요할듯
%===
%===
\begin{frame}
   \begin{assumpbox}{Assumption E}
        \( f_1, \ldots, f_J \) belong to \( C^s(\mathbb{R}) \) for some \( s > 0 \). Here, \( C^s(\mathbb{R}) \) denotes the space of locally Hölder-continuous functions equipped with the norm
        \[
        \| f \|_{C^s} = \| f \|_\infty + \sup_{0 < |x - y| \leq 1} \frac{|f(x) - f(y)|}{|x - y|^s}, \quad \text{for } 0 < s < 1,
        \]
        and for \( s \geq 1 \),
        \[
        \| f \|_{C^s} = \| f^{(\lfloor s \rfloor)} \|_{C^{s - \lfloor s \rfloor}} + \sum_{0 \leq i < \lfloor s \rfloor} \| f^{(i)} \|_\infty.
        \]
    \end{assumpbox}
    \begin{itemize}[label=\scalebox{0.5}{$\blacksquare$}]
    \item Typical smoothness condition
    
    \end{itemize}
\end{frame}
%===
%===
   \begin{frame}
    \begin{mytheorembox}{Theorem 5}
        Grant Assumptions B, C', D, and E. Suppose \( L_0 \to \infty \) as \( N \to \infty \), and \( L_0^{\max(5, (J+3)/2)} r_N \to 0 \), where \( r_N = \left( \frac{N}{\log N} \right)^{-\frac{s}{1 + 2s}} \). Then there exist estimators \( \hat{f}_j, 1 \leq j \leq J \) (continuous so that the supremum below is measurable) and a permutation \( \tau \) s.t., for some \( C > 0 \),
        \[
        \Pi_H \left( \| \hat{f}_j - f_{\tau(j)} \|_\infty \geq C L_0^5 r_N \right) \to 0.     \quad     (17)
        \]
        Convergence in expectation also holds: for some \( C' > 0 \),
        \[
        \mathbb{E}_H \| \hat{f}_j - f_{\tau(j)} \|_\infty \leq C' L_0^5 r_N.     \quad     (18)
        \]
    \end{mytheorembox}
    \begin{itemize}[label=\scalebox{0.5}{$\blacksquare$}]
    \item The supremum norm is indeed possible at a near-minimax rate in the nonparametric setting.
    \end{itemize}
\end{frame}
% 이어서 명제 6에서 HMM 세팅에서도 목표 수렴속도를 달성한다는 것을 시사 
% 정리5는 스펙트럼 방법, 정리6은 KDE로 알고리즘
%===
\section{Proofs}
%===
\begin{frame}{FDR Control, TDR Optimality}

    \begin{mytheorembox}{Lemma 9.}
        In the setting of Theorem 2, define \(\epsilon'_N = \epsilon_N (\log N)^u\), and recall that by definition \(u > 1 + \nu^{-1}\) and by assumption \(\epsilon'_N \to 0\), where \(\nu\) is the parameter of Assumption B. Then
        \[
        \max_{i \leq N} \Pi_H(|\hat{\ell}_i(X) - \ell_i(X)| > \epsilon'_N) \to 0, \quad \text{as} \ N \to \infty. \tag{20}
        \] 
        Consequently, there exists \(\delta_N \to 0\) such that
        \[
        \Pi_H\left(\#\{i \leq N : |\hat{\ell}_i(X) - \ell_i(X)| > \epsilon'_N\} > N \delta_N \right) \to 0.
        \]
    \end{mytheorembox}
    \begin{itemize}[label=\scalebox{0.5}{$\blacksquare$}]
        \item Lemma 9 shows that \(\hat{\ell}_i(X)\) converges to \(\ell_i(X)\) at a rate slightly slower than \(\epsilon_N\).
        \item This lemma is essential for establishing bounds on the FDR and TDR.
    \end{itemize}
\end{frame}


\begin{frame}{Proof}
   \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
        
        \item \textit{Initial Definitions and Assumptions}:
        \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
            \setlength{\itemsep}{\baselineskip}
            \item Define \(\epsilon'_N = \epsilon_N (\log N)^u\), with \(u > 1 + \nu^{-1}\) and assume \(\epsilon'_N \to 0\).
            \item \(\nu\) is the parameter of Assumption B.
            \item The goal is to prove that \(\max_{i \leq N} \Pi_H(|\hat{\ell}_i(X) - \ell_i(X)| > \epsilon'_N) \to 0\).
        \end{itemize}

        \item \textit{Setting Initial Bound for Each \(i\)}:
        \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
            \setlength{\itemsep}{\baselineskip}
            \item Show that for all \(i\), \(\Pi_H(|\hat{\ell}_i(X) - \ell_i(X)| > M \epsilon'_N) \to 0\), where \(M\) is a constant depending only on certain bounds for the parameter \(H = (Q, \pi, f_0, f_1)\).
        \end{itemize}
    \end{itemize}
\end{frame}
%===
\begin{frame}
\begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
    \setlength{\itemsep}{\baselineskip}
    \item \textit{Defining Probabilistic Event Sequence \((E_N)_N\)}:
        \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
            \setlength{\itemsep}{\baselineskip}
            \item Define a sequence of events \((E_N)_N\) with probability tending to 1 where the estimators \(\hat{Q}, \hat{\pi}, \hat{f}_j\) are close to the true values \(Q, \pi, f_j\) within \(\epsilon_N\).
            \item Introduce additional parameters \(\delta\) and \(\rho\) for further calculations.
        \end{itemize}

        \item \textit{Establishing the Main Inequality}:
        \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
            \setlength{\itemsep}{\baselineskip}
            \item Use Proposition 2.2 from De Castro et al. (2017) to establish an inequality for \(|\hat{\ell}_i(X) - \ell_i(X)|\) involving \(\epsilon_N\) and \(\epsilon'_N\).
            \item Derive the inequality by expressing \(\ell_i(X)\) and \(\hat{\ell}_i(X)\) in terms of the defined parameters.
        \end{itemize}

        
    \end{itemize}
\end{frame}
%===
\begin{frame}
\begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
    \item \textit{Combining Probabilistic Bounds}:
        \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
            \setlength{\itemsep}{\baselineskip}
            \item Split terms into intervals \(S_{\kappa,i}\) and \(S^c_{\kappa,i}\) to further refine the bounds.
            \item Show that the probability \(\Pi_H\) of exceeding certain bounds tends to zero by using union bounds.
        \end{itemize}
    \setlength{\itemsep}{\baselineskip}
    \item \textit{Applying Markov's Inequality}:
        \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
            \setlength{\itemsep}{\baselineskip}
            \item Apply Markov's inequality to show that the derived bounds hold for all \(i\) and extend to the entire sequence \(N\).
            \item Conclude that \(\max_{i} \Pi_H(|\hat{\ell}_i(X) - \ell_i(X)| > \epsilon'_N) \to 0\).
        \end{itemize} \qed
\end{itemize}
\end{frame}

%===
\begin{frame}{Pf of Thm 2}

    \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
        \setlength{\itemsep}{\baselineskip}
        \item \textit{Initial Definitions and Notation}:
            \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
                \setlength{\itemsep}{\baselineskip}
                \item Define \(\hat{t} = \text{postFDR}_{\hat{H}} \hat{\varphi}\).
                \item Let \(\hat{S}_0\) be the rejection set of \(\varphi\).
                \item Define \(\epsilon'_N\) as a sequence of positive numbers and \(F_N\) as a sequence of events.
            \end{itemize}

        \item \textit{Analyzing the Difference Between FDR and Expectation}:
            \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
                \setlength{\itemsep}{\baselineskip}
                \item Analyze \(|\text{FDR}_{H}(\hat{\varphi}) - E_{H} \hat{t}|\).
                \item Use the difference in expectations and bounds based on \(\hat{\ell}_i(X)\) and \(\ell_i(X)\).
            \end{itemize}
    \end{itemize}

\end{frame}
%===
%===
\begin{frame}
\begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
    \item \textit{Setting the Bound}:
            \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
                \setlength{\itemsep}{\baselineskip}
                \item Use \(| \ell_i(X) - \hat{\ell}_i(X) | \leq 1\) to set the bound.
                \item Apply Lemma 13: \(E_{H}[\hat{t}] \to \min(t, \pi_0)\).
            \end{itemize}

        \item \textit{Analyzing the Convergence of the Bound}:
            \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
                \setlength{\itemsep}{\baselineskip}
                \item Show that the right-hand side tends to zero for suitable \(\epsilon'_N\) and \(F_N\).
                \item Use Lemma 14: \( \Pi_{H}(|\hat{S}_0| > aN) \to 1 \) for some \(a > 0\).
                \item Combine with Lemma 9: choose \(\epsilon'_N \to 0\), \(\delta_N \to 0\), \(a > 0\).
            \end{itemize}
\end{itemize}
\end{frame}
%===


%===
\begin{frame}
    \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
                \item \textit{Defining the Event \(F_N\)}:
            \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
                \setlength{\itemsep}{\baselineskip}
                \item Define \(F_N\) as \(\{ \#\{i \leq N : |\hat{\ell}_i(X) - \ell_i(X)| > \epsilon'_N\} \leq N \delta_N \} \cap \{ |\hat{S}_0| > aN \}\).
                \item Show that \( \Pi_{H}(F_N^c) \to 0 \).
            \end{itemize}

        \item \textit{Deriving the Result}:
            \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
                \setlength{\itemsep}{\baselineskip}
                \item Show that 
                \[
                E_{H} \left[ \sum_{i=1}^{N} 1_{F_N} 1_{\{|\ell_i(X) - \hat{\ell}_i(X)| > \epsilon'_N\}} \bigg/ (1 \vee |\hat{S}_0|) \right] \leq \frac{N \delta_N}{aN} \to 0.
                \]
                \item Conclude that the final result holds, ensuring the convergence of the estimator.
            \end{itemize} \qed
    \end{itemize}
\end{frame}
%===

\begin{frame}{Pf of Thm 3}
    \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
        \setlength{\itemsep}{\baselineskip}
        \item \textit{Initial Definitions and Notation}:
            \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
                \setlength{\itemsep}{\baselineskip}
                \item Define \(\lambda^*\) as in Lemma 10.
                \item If \(\lambda^* = 1\), show that \(\hat{\varphi}\) rejects all but \(o_p(N)\) of the hypotheses.
                \item Assume \(\lambda^* < 1\), or equivalently \(t < \pi_0\).
            \end{itemize}

        \item \textit{Comparison with 'Oracle' Procedure}:
            \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
                \setlength{\itemsep}{\baselineskip}
                \item Compare \(\hat{\varphi}\) with the 'oracle' procedure \(\varphi_{\lambda^*, H}\).
                \item For \(\epsilon_N > 0\), decompose:
                \[
                1\{\ell_i < \lambda^*\} \leq 1\{\lambda^* - \epsilon_N \leq \ell_i < \lambda^*\} + 1\{\hat{\ell}_i < \hat{\lambda}\} + 1\{\hat{\lambda} < \lambda^* - \epsilon_N/2\} + 1\{\hat{\ell}_i - \ell_i > \epsilon_N/2\}.
                \]
            \end{itemize}

        
    \end{itemize}
\end{frame}

%===
\begin{frame}
    \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
        \item \textit{Convergence of Estimators}:
            \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
                \setlength{\itemsep}{\baselineskip}
                \item Use Lemma 10 to show \(\hat{\lambda}\) converges to \(\lambda^*\) in probability.
                \item Use Lemma 9 to show \(\#\{i : |\hat{\ell}_i - \ell_i| > \epsilon_N/2\}/N \to 0\).
            \end{itemize}

        \item \textit{Analyzing Decomposed Terms}:
            \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
                \setlength{\itemsep}{\baselineskip}
                \item Use Lemma 19: \(\xi_N \to 0\) exists such that \(\#\{i : |\ell_i - \ell^\infty_i| > \xi_N\}/N \to 0\).
                \item Use Lemma 17: the distribution function of \(\ell^\infty_i\) is continuous:
                \begin{align*}
                E_H\left[\#\{i : \lambda^* - \epsilon_N \leq \ell_i < \lambda^*\}/N\right] 
                &\leq E_H\left[\#\{i : |\ell_i - \ell^\infty_i| > \xi_N\}/N\right] \\
                &\quad + \Pi_H(\lambda^* - \epsilon_N - \xi_N \leq \ell^\infty_i < \lambda^* + \xi_N) \to 0.
                \end{align*}
            \end{itemize}

        \item \textit{mTDR Analysis}:
            \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
                \setlength{\itemsep}{\baselineskip}
                \item Derive:
                \[
                E_H[\#\{i : \theta_i = 1, \hat{\ell}_i < \hat{\lambda}\}] \geq E_H[\#\{i : \theta_i = 1, \ell_i < \lambda^*\}] - o(N).
                \]
                \item Divide both sides by \(E_H[\#\{i : \theta_i = 1\}] = N\pi_1\):
                \[
                mTDR_H(\hat{\varphi}) \geq mTDR_H(\varphi_{\lambda^*, H}) - o(1).
                \]
            \end{itemize}

        
    \end{itemize}
\end{frame}
%===
%===
\begin{frame}
    \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
        \item \textit{mFDR Analysis}:
            \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
                \setlength{\itemsep}{\baselineskip}
                \item Derive similarly:
                \[
                E_H[\#\{i : \theta_i = 0, \hat{\ell}_i < \hat{\lambda}\}] \leq E_H[\#\{i : \theta_i = 0, \ell_i < \lambda^*\}] + o(N),
                \]
                \[
                E_H[\#\{i : \hat{\ell}_i < \hat{\lambda}\}] \geq E_H[\#\{i : \ell_i < \lambda^*\}] - o(N).
                \]
                \item Use Taylor expansion:
                \[
                mFDR_H(\hat{\varphi}) \leq mFDR_H(\varphi_{\lambda^*, H}) + o(1).
                \]
            \end{itemize}

        \item \textit{Final Result}:
            \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
                \setlength{\itemsep}{\baselineskip}
                \item Define \(g(x) = \sup\{mTDR_H(\psi) : mFDR_H(\psi) \leq x\}\):
                \begin{align*}
                mTDR_H(\hat{\varphi}) &\geq mTDR_H(\varphi_{\lambda^*, H}) - o(1) \\
                &\geq g(mFDR_H(\varphi_{\lambda^*, H})) - o(1) \\
                &\geq g(mFDR_H(\hat{\varphi}) - o(1)) - o(1) \\
                &\geq g(mFDR_H(\hat{\varphi})) - o(1).
                \end{align*}
                \item Prove that \(mFDR_H(\varphi_{\lambda^*, H}) \geq t - o(1)\). \qed
            \end{itemize}
    \end{itemize}
\end{frame}
%===

%===
\begin{frame}{Spectral KDE method}
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.8\textwidth]{fig-2/KDE.png}
    \end{figure}


    \begin{itemize}[label=\scalebox{0.5}{$\blacksquare$}]
        \item KDE is a nonparametric method. (kernel smoothing)
        \item Spectral method → eigenvalue
    \end{itemize}
\end{frame}
%===
\begin{frame}
    \begin{mytheorembox}{Approximating emission density}
    Let \(K\) be a bounded Lipschitz-continuous function, supported in \([-1,1]\), such that if we define
    \begin{equation}
        K_L(x,y) = 2^L K(2^L(x-y)), \tag{25}
        \end{equation}
    then we have, for any \(f \in C^s(\mathbb{R})\),
    \begin{equation*}
        K_L[f](x) = \int K_L(x,y) f(y) \, dy,
        \end{equation*}
    with
    \begin{equation}
        \|f - K_L[f]\|_\infty \leq C \|f\|_{C^s} 2^{-Ls}. \tag{26}
        \end{equation}
    \end{mytheorembox}
    \begin{itemize}[label=\scalebox{0.5}{$\blacksquare$}]
        \vspace{1.5em}
        \itemsep1.2em
        \item By using (26), We can estimate emission density!
        \item I'll explain an example idea on the next page.
    \end{itemize}
\end{frame}
%===
\begin{frame}{Idea example}
    \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
        \item Joint distribution and define matrices
        \[
        M_x = \mathbb{E}_H
        \begin{pmatrix}
        K_L(x, X_2) & K_L(x, X_2) \mathbf{1}\{X_3 \in [-1, 1]\} \\
        K_L(x, X_2) \mathbf{1}\{X_1 \in [-1, 1]\} & K_L(x, X_2) \mathbf{1}\{X_1 \in [-1, 1], X_3 \in [-1, 1]\}
        \end{pmatrix},
        \]

        \[
        P = \mathbb{E}_H
        \begin{pmatrix}
        1 & \mathbf{1}\{X_3 \in [-1, 1]\} \\
        \mathbf{1}\{X_1 \in [-1, 1]\} & \mathbf{1}\{X_1 \in [-1, 1], X_3 \in [-1, 1]\}
        \end{pmatrix}.
        \]
        
        \item Take eigenvalues \\
        \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
            \item The matrices \( P \) and \( M_x \) esimated empirical aveg.
            \item Define \( B_x = P^{-1} M_x \) and estimate parameter from eigenvalues \(K_L[f_j]\)
        \end{itemize}
        \item Simultaneous Diagonalization \\
        \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
            \item we can applicate above general setting $J > 1$
        \end{itemize}
    \end{itemize}
\end{frame}


%===
\begin{frame}{Lemma}

\begin{mytheorembox}{Lemma 11.}
 For \(L_0 \in \mathbb{N}\), let \(h_1, \ldots, h_{L_0}\) be arbitrary functions. Define, for data \(X\) from the HMM (1):

\begin{align*}
M_x &:= \left(E_H\left[h_l(X_1) K_L(x, X_2) h_m(X_3)\right]\right)_{l,m \leq L_0} \in \mathbb{R}^{L_0 \times L_0}, \\
P &:= \left(E_H\left[h_l(X_1) h_m(X_3)\right]\right)_{l,m \leq L_0} \in \mathbb{R}^{L_0 \times L_0}, \\
D_x &:= \text{diag}\left(K_L[f_j](x)_{j \leq J}\right) \in \mathbb{R}^{J \times J}, \\
O &:= \left(E_H\left[h_l(X_1) \mid \theta_1 = j\right]\right)_{l \leq L_0, j \leq J} \in \mathbb{R}^{L_0 \times J}.
\end{align*}

\end{mytheorembox}
   \begin{itemize}[label=\scalebox{0.5}{$\blacksquare$}]
        \item The proof is a conditional technique learned from stochastic process.
    \end{itemize}
\end{frame}
%===

%===
\begin{frame}

    \begin{mytheorembox}{Contd.}
    Then,
    \begin{align*}
    M_x &= O \ \text{diag}(\pi) Q D_x Q O^T, \\
    P &= O \ \text{diag}(\pi) Q^2 O^T.
    \end{align*}
    
    If \(V \in \mathbb{R}^{L_0 \times J}\) is such that \(V P V\) is \textbf{invertible}, then the matrix
    
    \begin{align*}
    B_x &:= (V P V)^{-1} V M_x V
    \end{align*}
    
    satisfies
    
    \begin{align*}
    B_x &= (Q O V)^{-1} D_x (Q O V),
    \end{align*}
    \end{mytheorembox}
    \begin{itemize}[label=\scalebox{0.5}{$\blacksquare$}]

        \item B : \textit{diagonaalisable simultaneously}
        
    \end{itemize}
\end{frame}
%===


%===
\begin{frame}{Algorithm 1}
    \begin{mytheorembox}{Input}
        \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
            \item Data : $X_n : n \leq N+2$ in HMM
            \item Functions $h_1, \ldots, h_{L_0}$, uniformly bounded, s.t. $O = (\mathbb{E}[h_l(X_1) \mid \theta_1 = j])_{l \leq L_0, j \leq J}$
            \item Finite sets $D_N \subseteq \{(a, u) \in \mathbb{R}^{J(J-1)/2} \times \mathbb{R}^{J(J-1)/2} : \sum |a_i| \leq 1\}$ s.t.
            \[
            \max_{(a, u) \in D_N} \text{sep}(B_{a, u})
            \]
            where $B_{a, u} = \sum a_i B_{u_i}$ for $B_x$ for some $V$.
        \end{itemize}
        
    \end{mytheorembox}
\end{frame}
%===
%===
\begin{frame}
    \begin{mytheorembox}{estimate}
        \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
             \item \textbf{Estimate} the matrices $P$, $(M_x, x \in \mathbb{R})$ by taking empirical averages. For $L$ s.t. $2L \approx \left(\frac{N}{\log N}\right)^{\frac{1}{1+2s}}$, define
            \[
            \hat{P} = \hat{P}_{L_0} = \left(N^{-1} \sum_{n \leq N} h_l(X_n) h_m(X_{n+2})\right)_{l, m \leq L_0},
            \]
            \[
            \hat{M}_x = \hat{M}_{x, L_0, L} = \left(N^{-1} \sum_{n \leq N} h_l(X_n) K_L(x, X_{n+1}) h_m(X_{n+2})\right)_{l, m \leq L_0}.
            \]
        
            \item Let $\hat{V} = \hat{V}_{L_0} \in \mathbb{R}^{L_0 \times J}$ be a matrix of orthonormal right singular vectors of $\hat{P}$ (fail if $\hat{P}$ is of rank less than $J$).
            
        \end{itemize}
        
    \end{mytheorembox}
    \begin{itemize}[label=\scalebox{0.5}{$\blacksquare$}]

        \item Why fail if less than rank J ? : information loss.
        \item I think $\hat{P}$ and $\hat{M}_x$ describe the relationships between different nodes.
        
        \end{itemize}
\end{frame}
%===


%===
\begin{frame}
    \begin{mytheorembox}{Set and Choose}
        \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
    
        \item \textbf{Set}, for $x \in \mathbb{R}$ and for $a, u \in \mathbb{R}^{J(J-1)/2}$,
        \[
        \hat{B}_x = \hat{B}_{x, L_0, L} := (\hat{V} \hat{P} \hat{V})^{-1} \hat{V} \hat{M}_x \hat{V} \text{ (fail if } \hat{B}_{a, u} \text{ is not diagonalisable)}.
        \]
    
        \item \textbf{Choose} $\hat{R}$ of normalised columns diagonalising $\hat{B}_x$.
    \end{itemize}
        
    \end{mytheorembox}
\end{frame}
%===


%===
\begin{frame}
    \begin{mytheorembox}{Output}
    \begin{itemize}[label=\scalebox{0.5}{$\bullet$}]
        \item Output $(\hat{f}_j : j \leq J)$, where, defining $\tilde{M}_x \hat{V}$,
        \[
        \hat{B}_{a, u} := \sum a_i \hat{B}_{u_i},
        \]
        with $(\hat{a}, \hat{u}) \in \arg\max_{D_N} \text{sep}(\hat{B}_{a, u})$, and
        \[
        \tilde{f}^L_j(x) = (\hat{R}^{-1} \hat{B}_x \hat{R})_{jj},
        \]
        we set
        \[
        \hat{f}_j(x) =
        \begin{cases}
        \tilde{f}^L_j(x) & \text{if } |\tilde{f}^L_j(x)| \leq N^\alpha, \\
        N^\alpha \operatorname{sign}(\tilde{f}^L_j(x)) & \text{otherwise},
        \end{cases}
        \]
        for arbitrary $\alpha > 0$.
    \end{itemize}
        
    \end{mytheorembox}
\end{frame}
%===


%===
\begin{frame}

\end{frame}
%===


%===
\begin{frame}

\end{frame}
%===

%===
\begin{frame}

\end{frame}
%===


%===
\begin{frame}

\end{frame}
%===


%===
\begin{frame}

\end{frame}
%===
\end{document}
